{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b5d6d6fa4454861ae5d3839b4146dcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from scipy.sparse import hstack,vstack\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import taskALoader as t_a\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC,SVC\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV,ShuffleSplit\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from langdetect import detect\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "tqdm().pandas()\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#man_anntd = pd.read_csv(\"manually_annotated_risk.csv\").fillna(' ')\n",
    "man_anntd = pd.read_csv(\"manually_annotated_risk_2.csv\").fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#man_anntd = man_anntd.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#man_anntd = man_anntd.head(247)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5277310924369748"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab = man_anntd['risk_labels']\n",
    "lab = list(lab)\n",
    "lab.count(0)/float(len(lab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(man_anntd['list_of_posts'])\n",
    "y = list(man_anntd['risk_labels'])\n",
    "y = np.asarray(y, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5182186234817814\n"
     ]
    }
   ],
   "source": [
    "print(128/(128+119))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_all, X_test, y_train_all, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val  = train_test_split(X_train_all, y_train_all, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357 119\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train),len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df = pd.read_csv(\"clpsych19_training_data/shared_task_posts.csv\").fillna(' ')\n",
    "p_title = corpus_df['post_title']\n",
    "p_body = corpus_df['post_body']\n",
    "corpus_text = pd.concat([p_title,p_body]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(p_body[100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Do I have to look for manual for it? Buy a new one? Or what?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_body[1385]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nc = 0\\nfor i,body in tqdm(enumerate(p_body)):\\n    #print(i)\\n    text = p_title[i]+\" \"+body\\n    if(len(text.strip())!=0):\\n        if(detect(str(text))!=\\'en\\'):\\n            print(text,detect(text))\\n            c = c + 1\\n            \\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "c = 0\n",
    "for i,body in tqdm(enumerate(p_body)):\n",
    "    #print(i)\n",
    "    text = p_title[i]+\" \"+body\n",
    "    if(len(text.strip())!=0):\n",
    "        if(detect(str(text))!='en'):\n",
    "            print(text,detect(text))\n",
    "            c = c + 1\n",
    "            \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predefined_split(features_train, features_val, target_train, target_val):\n",
    "    always_train = np.empty((target_train.shape[0]), dtype=np.int32)\n",
    "    #print(target_train.shape[0])\n",
    "    always_train[:] = -1\n",
    "    always_validate = np.empty((target_val.shape[0]), dtype=np.int32)\n",
    "    always_validate[:] = 0\n",
    "    pred_split_indices = np.concatenate((always_train, always_validate))\n",
    "    curr_target_train = np.concatenate((target_train, target_val))\n",
    "    #curr_features_train = vstack((features_train, features_val))\n",
    "    curr_features_train = np.concatenate((features_train, features_val))\n",
    "   \n",
    "    ps = PredefinedSplit(test_fold=pred_split_indices)\n",
    "\n",
    "    return curr_features_train, curr_target_train, ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfVectorizerWrapper(TfidfVectorizer):\n",
    "    def fit(self,x,y=None, **fit_params):\n",
    "        x = corpus_text\n",
    "        return super(TfidfVectorizerWrapper, self).fit(x, y, **fit_params)\n",
    "        \n",
    "    def transform(self, x, y=None, **fit_params):\n",
    "        #x = [content.split('\\t')[0] for content in x]  # filtering the input\n",
    "        return super(TfidfVectorizerWrapper, self).transform(x, y, **fit_params)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_param_selection(X_train,X_val,y_train,y_val):\n",
    "    \n",
    "\n",
    "    \n",
    "    linear_pipeline = Pipeline(\n",
    "        [\n",
    "            ('tfidf',\n",
    "             TfidfVectorizerWrapper(decode_error='ignore', use_idf=True, ngram_range=(1, 2), lowercase=True,\n",
    "                             stop_words='english',\n",
    "                             analyzer='word', max_features=40000)),\n",
    "            ('classifier', LogisticRegression(class_weight='balanced',max_iter=2000))]\n",
    "    )\n",
    "\n",
    "    \n",
    "    \n",
    "    ngram_range=[(1, 2), (1, 3), (1, 4), (1, 5)]\n",
    "    #max_features=[100000, 200000, 300000, 400000, 500000]\n",
    "    max_features = [200000,500000,600000,700000]\n",
    "    min_df=[1, 2, 3]\n",
    "    \n",
    "    \n",
    "    \n",
    "    X,y,ps = get_predefined_split (X_train,X_val,y_train,y_val)\n",
    "    \n",
    "    #Logistic Regression hyperparameters\n",
    "    #penalty = ['l2']\n",
    "    penalty = ['l1','l2']\n",
    "    Cs = [0.001, 0.01, 0.1, 1, 10]\n",
    "    #multi_class = ['multinomial','ovr']\n",
    "    multi_class = ['auto']\n",
    "    #solver = ['newton-cg', 'lbfgs', 'sag']\n",
    "    solver = ['liblinear']\n",
    "    param_grid = {\"tfidf__ngram_range\": ngram_range,\n",
    "                  \"tfidf__use_idf\": [True],\n",
    "                  \"tfidf__max_features\": max_features,\n",
    "                  \"tfidf__min_df\": min_df,\n",
    "                  \"tfidf__sublinear_tf\": [True],\n",
    "                  \"classifier__C\": Cs,\n",
    "                  \"classifier__multi_class\":multi_class,\n",
    "                  \"classifier__solver\":solver,\n",
    "                  \"classifier__penalty\":penalty\n",
    "                 \n",
    "                 }\n",
    "                             \n",
    "    \n",
    "   \n",
    "\n",
    "    grid_search = GridSearchCV(linear_pipeline, param_grid,cv=ps,scoring='f1_macro', refit=False)\n",
    "\n",
    "    clf=grid_search.fit(X, y)\n",
    " \n",
    "    \n",
    "    grid_search.best_params_\n",
    "    return grid_search.best_params_,clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "best_pars,model = logistic_param_selection(X_train,X_val,y_train,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runBestLogistic(train,train_labels,test,test_labels,best_pars):\n",
    "    \n",
    "    \n",
    "    c = best_pars['classifier__C']\n",
    "    multi_class = best_pars['classifier__multi_class']\n",
    "    penalty = best_pars['classifier__penalty']\n",
    "    solver = best_pars['classifier__solver']\n",
    "    \n",
    "    \n",
    "    max_features = best_pars['tfidf__max_features']\n",
    "    min_df = best_pars['tfidf__min_df']\n",
    "    ngram_range=best_pars['tfidf__ngram_range']\n",
    "    sublinear_tf = best_pars['tfidf__sublinear_tf']\n",
    "    use_idf = best_pars['tfidf__use_idf']\n",
    "    \n",
    "    \n",
    "    \n",
    "    linear_pipeline = Pipeline(\n",
    "        [\n",
    "            ('tfidf',\n",
    "             TfidfVectorizerWrapper(decode_error='ignore', \n",
    "                                    ngram_range=ngram_range, lowercase=True,stop_words='english',\n",
    "                             analyzer='word', max_features=max_features,min_df=min_df,\n",
    "                                    sublinear_tf=sublinear_tf,use_idf=use_idf)),\n",
    "            \n",
    "            ('classifier', LogisticRegression(C=c,penalty = penalty,solver=solver,multi_class=multi_class,class_weight='balanced',max_iter=2000))]\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #clf = LinearSVC(C=0.01, multi_class='crammer_singer',class_weight='balanced')\n",
    "    #clf = LogisticRegression()\n",
    "    #clf = SVC()\n",
    "    #clf = linear_pipeline()\n",
    "    #clf.fit(train, train_labels)\n",
    "    linear_pipeline.fit(train,train_labels)\n",
    "    predicted=linear_pipeline.predict(test)\n",
    "    print(classification_report(test_labels, predicted))\n",
    "    print (\"Accuracy: {}\".format(accuracy_score(test_labels, predicted)))\n",
    "    return linear_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier__C': 10, 'classifier__multi_class': 'auto', 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'tfidf__max_features': 200000, 'tfidf__min_df': 2, 'tfidf__ngram_range': (1, 2), 'tfidf__sublinear_tf': True, 'tfidf__use_idf': True}\n"
     ]
    }
   ],
   "source": [
    "print(best_pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.80      0.84        65\n",
      "         1.0       0.78      0.87      0.82        54\n",
      "\n",
      "   micro avg       0.83      0.83      0.83       119\n",
      "   macro avg       0.83      0.84      0.83       119\n",
      "weighted avg       0.84      0.83      0.83       119\n",
      "\n",
      "Accuracy: 0.8319327731092437\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.77      0.81        62\n",
      "         1.0       0.77      0.84      0.81        57\n",
      "\n",
      "   micro avg       0.81      0.81      0.81       119\n",
      "   macro avg       0.81      0.81      0.81       119\n",
      "weighted avg       0.81      0.81      0.81       119\n",
      "\n",
      "Accuracy: 0.8067226890756303\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizerWrapper(analyzer='word', binary=False, decode_error='ignore',\n",
       "            dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "            input='content', lowercase=True, max_df=1.0,\n",
       "            max_features=200000, min_df=2, ngram_range=(1, 2), norm='l2',\n",
       "            pr...ty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = runBestLogistic(X_train,y_train,X_val,y_val,best_pars)\n",
    "runBestLogistic(X_train,y_train,X_test,y_test,best_pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        65\n",
      "         1.0       1.00      1.00      1.00        54\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       119\n",
      "   macro avg       1.00      1.00      1.00       119\n",
      "weighted avg       1.00      1.00      1.00       119\n",
      "\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = runBestLogistic(X,y,X_val,y_val,best_pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = '605_Lb_logi_binary_clf_model_all_data.sav'\n",
    "\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
